{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Public Transport Data Dashboard\n",
    "\n",
    "![mrt jakarta](plugins/assets/mrt_jakarta.jpg)\n",
    "\n",
    "### Objective:\n",
    "The aim of this project is to fetch real-time or historical data about public transport systems (e.g., buses, trains, or subways) from open APIs or datasets. The gathered data will be processed, cleaned, and visualized through an interactive dashboard. The dashboard will help visualize patterns such as transport availability, punctuality, routes, and passenger demand over time.\n",
    "\n",
    "## Key Features of the Project:\n",
    "\n",
    "### Data Collection:\n",
    "Identify reliable open sources for public transport data (e.g., city transport APIs, GTFS feeds, or public transport websites).\n",
    "Use Python to fetch data using libraries such as requests, pandas, or openpyxl.\n",
    "\n",
    "\n",
    "### Data Processing:\n",
    "Clean and preprocess the data to ensure it's in a usable format.\n",
    "Handle missing data, duplicates, and irrelevant columns.\n",
    "Perform any necessary transformations (e.g., timestamp conversions, geospatial coordinates for locations).\n",
    "\n",
    "### Data Storage:\n",
    "Store the data in a local database (e.g., SQLite) or a cloud-based data warehouse (e.g., Google BigQuery, AWS Redshift) for later use.\n",
    "\n",
    "\n",
    "\n",
    "### Dashboard Development:\n",
    "Use a Python visualization library (e.g., Plotly, Dash, Matplotlib) to build an interactive dashboard.\n",
    "The dashboard will allow users to interact with data, filter by transport type, and visualize transport routes, schedules, or other metrics.\n",
    "\n",
    "\n",
    "### Experimentation & Analysis:\n",
    "Experiment with data fetching, transformation, and the integration of APIs.\n",
    "Explore possible analyses such as peak-hour transport usage, performance (on-time arrivals), and comparison across routes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import plugins.utils as utils\n",
    "import snowflake.connector\n",
    "from plugins.config import snow_creds, aws_creds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "# Don't forget to setup your kaggle user on .../Users/youruser/.kaggle/kaggle.json\n",
    "\n",
    "\n",
    "dataset_name = \"pablodiegoo/analysis-of-chicago-divvy-bicycle-sharing-updated\" \n",
    "download_folder = \"./plugins/assets/data/divvy_tripdata\"\n",
    "utils.download_kaggle_dataset(dataset_name, download_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into dataframe\n",
    "df = utils.import_csvs_and_merge(\"./plugins/assets/data/divvy_tripdata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_pop_df = utils.import_csvs_and_merge(\"./plugins/assets/data/ev_population/\")\n",
    "ev_pop_df.rename(str.lower, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ev_pop_df = ev_pop_df[[\"county\", \"city\", \"state\"]]\n",
    "ev_pop_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from Google Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run this client, you need to setup your account 1st on your local\n",
    "# Ref: https://cloud.google.com/sdk/docs/install, https://cloud.google.com/bigquery/docs/authentication/getting-started\n",
    "# Testing bigquery connection\n",
    "client = bigquery.Client()\n",
    "query = \"\"\"\n",
    "    SELECT * FROM `bigquery-public-data.austin_bikeshare.bikeshare_trips` LIMIT 5\n",
    "\"\"\"\n",
    "query_job = client.query(query)\n",
    "df = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(**snow_creds)\n",
    "\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM public_transport.transport_base.example\")\n",
    "result = cur.fetch_pandas_all()\n",
    "print(result)\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the handler instance\n",
    "s3_handler = utils.S3ParquetHandler(aws_creds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your S3 bucket and file keys\n",
    "destination_bucket = \"project-etl-iqbal\"\n",
    "destination_key = \"etl/iqbal_test.parquet\"\n",
    "# Write the DataFrame back to S3 as a Parquet file\n",
    "s3_handler.write_parquet_to_s3(ev_pop_df[[\"county\", \"city\", \"state\"]], destination_bucket, destination_key)\n",
    "print(f\"Data successfully written to s3://{destination_bucket}/{destination_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your S3 bucket and file keys\n",
    "source_bucket = \"project-etl-iqbal\"\n",
    "source_key = \"etl/iqbal_test.parquet\"\n",
    "# Read the Parquet file from S3 into a DataFrame\n",
    "df = s3_handler.read_parquet_from_s3(source_bucket, source_key)\n",
    "print(\"Data read from S3:\")\n",
    "print(df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
